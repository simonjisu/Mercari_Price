{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('../data/test.tsv', sep='\\t')\n",
    "\n",
    "# train\n",
    "df_cate = df_train.category_name.str.split('/', expand=True, n=2).fillna(np.nan)\n",
    "df_cate.columns = ['cate_b', 'cate_m', 'cate_s']  # category big, middle, small\n",
    "df_train = pd.concat([df_train, df_cate], axis=1)\n",
    "df_train['has_brand'] = (~df_train.brand_name.isnull()).astype(int)\n",
    "\n",
    "# test\n",
    "df_cate = df_test.category_name.str.split('/', expand=True, n=2).fillna(np.nan)\n",
    "df_cate.columns = ['cate_b', 'cate_m', 'cate_s']  # category big, middle, small\n",
    "df_test = pd.concat([df_test, df_cate], axis=1)\n",
    "df_test['has_brand'] = (~df_test.brand_name.isnull()).astype(int)\n",
    "\n",
    "cate_b_unique = df_train.cate_b.unique()\n",
    "cate_b_dict = {cat: i for i, cat in enumerate(cate_b_unique, 1)}\n",
    "cate_b_dict[np.nan] = 0\n",
    "cate_m_unique = df_train.cate_m.unique()\n",
    "cate_m_dict = {cat: i for i, cat in enumerate(cate_m_unique, 1)}\n",
    "cate_m_dict[np.nan] = 0\n",
    "cate_s_unique = df_train.cate_s.unique()\n",
    "cate_s_dict = {cat: i for i, cat in enumerate(cate_s_unique, 1)}\n",
    "cate_s_dict[np.nan] = 0\n",
    "\n",
    "use_cols = ['price', 'item_condition_id', 'has_brand', 'shipping', 'cate_b', 'cate_m', 'cate_s']\n",
    "df = df_train.copy()\n",
    "df = df.loc[:, use_cols]\n",
    "df_t = df_test.copy()\n",
    "df_t = df_t.loc[:, use_cols[1:]]\n",
    "\n",
    "# check if category data is in the test data\n",
    "def check_cate(test_cates, unique):\n",
    "    c_list = []\n",
    "    for c in test_cates:\n",
    "        if ((unique == c).sum() == 0) & (type(c) == str):\n",
    "            c_list.append(c)\n",
    "    return c_list\n",
    "    \n",
    "for cat, cat_unique in zip(['cate_b', 'cate_m', 'cate_s'], \n",
    "                           [cate_b_unique, cate_m_unique, cate_s_unique]):\n",
    "    print(cat)\n",
    "    print('='*30)\n",
    "    not_in_list = check_cate(df_test[cat].unique(), cat_unique)\n",
    "    print('num of categories that are not in test data:', len(not_in_list))\n",
    "    if not_in_list:\n",
    "        print(not_in_list)\n",
    "    print('='*30)\n",
    "\n",
    "# change categories to numbers\n",
    "for cat, cat_dict in zip(['cate_b', 'cate_m', 'cate_s'], [cate_b_dict, cate_m_dict, cate_s_dict]):\n",
    "    df[cat] = df[cat].map(cat_dict.get)\n",
    "    df_t[cat] = df_t[cat].map(cat_dict.get)\n",
    "    df_t[cat].loc[df_t[cat].isnull()] = 0\n",
    "    df_t[cat] = df_t[cat].astype(int)\n",
    "    \n",
    "    \n",
    "# batch size and data setting\n",
    "BATCH_SIZE = 512\n",
    "data_train = df.values\n",
    "data_test = df_t.values\n",
    "\n",
    "class CustomDataset(torchdata.Dataset):\n",
    "    def __init__(self, data, train=True):\n",
    "        n_row, n_col = data.shape\n",
    "        self.x = torch.FloatTensor(data[:, 1:]).contiguous().view(-1, n_col-1)\n",
    "        self.y = torch.FloatTensor(np.log(data[:, :1] + 1)).contiguous().view(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # return index of batch size\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # lenth of data set\n",
    "        return len(self.x)\n",
    "        \n",
    "train_dataset = CustomDataset(data=data_train)\n",
    "train_loader = torchdata.DataLoader(dataset=train_dataset,\n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   drop_last=True)\n",
    "                                   \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, I, H, O):\n",
    "        super(Network, self).__init__()\n",
    "        self.I = I\n",
    "        self.H1 = H[0]\n",
    "        self.H2 = H[1]\n",
    "        self.O = O\n",
    "        \n",
    "        self.l1 = nn.Linear(self.I, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.H2)\n",
    "        self.l3 = nn.Linear(self.H2, self.O)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.H1)\n",
    "        self.bn2 = nn.BatchNorm1d(self.H2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bn1(self.l1(inputs))\n",
    "        outputs = self.activation(outputs)\n",
    "        outputs = self.bn2(self.l2(outputs))\n",
    "        outputs = self.activation(outputs)\n",
    "        \n",
    "        return torch.log(self.l3(outputs) + 1)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        outputs = self.forward(inputs)\n",
    "        return torch.exp(outputs) - 1\n",
    "\n",
    "# building model\n",
    "EPOCH=10\n",
    "LR=0.1\n",
    "I = 6\n",
    "\n",
    "model = Network(I, [40, 20], 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# training model\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, (inputs, lg_targets) in enumerate(train_loader):\n",
    "        inputs, lg_targets = Variable(inputs).view(-1, I), Variable(lg_targets)\n",
    "\n",
    "        model.zero_grad()\n",
    "        lg_outputs = model.forward(inputs)\n",
    "        loss = loss_function(lg_outputs, lg_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        if i % 500 == 0:\n",
    "            print(\"[{0}/{1}] [{2}/{3}] mean_loss : {4:.3f}\"\\\n",
    "                  .format(epoch, EPOCH, i, len(train_loader), np.mean(losses)))\n",
    "            losses=[]\n",
    "\n",
    "data_test = torch.FloatTensor(data_test).contiguous().view(-1, 6)\n",
    "model.eval()\n",
    "pred = model.predict(Variable(test))\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission_stg2.csv', index_col='test_id')\n",
    "sub['price'] = pred.data.numpy()\n",
    "sub.to_csv('./data/torch_model.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
