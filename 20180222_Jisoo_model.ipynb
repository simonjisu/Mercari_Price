{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('./data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cate = df_train.category_name.str.split('/', expand=True, n=2).fillna(np.nan)\n",
    "df_cate.columns = ['cate_b', 'cate_m', 'cate_s']  # category big, middle, small\n",
    "df_train = pd.concat([df_train, df_cate], axis=1)\n",
    "df_train['has_brand'] = (~df_train.brand_name.isnull()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cate = df_test.category_name.str.split('/', expand=True, n=2).fillna(np.nan)\n",
    "df_cate.columns = ['cate_b', 'cate_m', 'cate_s']  # category big, middle, small\n",
    "df_test = pd.concat([df_test, df_cate], axis=1)\n",
    "df_test['has_brand'] = (~df_test.brand_name.isnull()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cate_b_unique = df_train.cate_b.unique()\n",
    "cate_b_dict = {cat: i for i, cat in enumerate(cate_b_unique, 1)}\n",
    "cate_b_dict[np.nan] = 0\n",
    "cate_m_unique = df_train.cate_m.unique()\n",
    "cate_m_dict = {cat: i for i, cat in enumerate(cate_m_unique, 1)}\n",
    "cate_m_dict[np.nan] = 0\n",
    "cate_s_unique = df_train.cate_s.unique()\n",
    "cate_s_dict = {cat: i for i, cat in enumerate(cate_s_unique, 1)}\n",
    "cate_s_dict[np.nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cols = ['price', 'item_condition_id', 'has_brand', 'shipping', 'cate_b', 'cate_m', 'cate_s']\n",
    "df = df_train.copy()\n",
    "df = df.loc[:, use_cols]\n",
    "df_t = df_test.copy()\n",
    "df_t = df_t.loc[:, use_cols[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if category data is in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_cate(test_cates, unique):\n",
    "    c_list = []\n",
    "    for c in test_cates:\n",
    "        if ((unique == c).sum() == 0) & (type(c) == str):\n",
    "            c_list.append(c)\n",
    "    return c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cate_b\n",
      "==============================\n",
      "num of categories that are not in test data: 0\n",
      "==============================\n",
      "cate_m\n",
      "==============================\n",
      "num of categories that are not in test data: 0\n",
      "==============================\n",
      "cate_s\n",
      "==============================\n",
      "num of categories that are not in test data: 12\n",
      "['Album', 'Amigurumi', 'Pretend', 'Professional & Trade', 'Tandem', 'Home', 'Fiber Art', 'Portraits', 'Rails & Rail Guards', 'Bedroom', 'Computer', 'Rugs']\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for cat, cat_unique in zip(['cate_b', 'cate_m', 'cate_s'], \n",
    "                           [cate_b_unique, cate_m_unique, cate_s_unique]):\n",
    "    print(cat)\n",
    "    print('='*30)\n",
    "    not_in_list = check_cate(df_test[cat].unique(), cat_unique)\n",
    "    print('num of categories that are not in test data:', len(not_in_list))\n",
    "    if not_in_list:\n",
    "        print(not_in_list)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Soo/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for cat, cat_dict in zip(['cate_b', 'cate_m', 'cate_s'], [cate_b_dict, cate_m_dict, cate_s_dict]):\n",
    "    df[cat] = df[cat].map(cat_dict.get)\n",
    "    df_t[cat] = df_t[cat].map(cat_dict.get)\n",
    "    df_t[cat].loc[df_t[cat].isnull()] = 0\n",
    "    df_t[cat] = df_t[cat].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = df.iloc[:, 1:].values\n",
    "y_train = df.iloc[:, :1].values\n",
    "# y_train = np.log(y_train + 1)\n",
    "X_test = df_t.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model: RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv', index_col='test_id')\n",
    "sub['price'] = np.exp(pred) - 1\n",
    "sub.to_csv('./data/random_forest_regressor.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv('./data/random_forest_regressor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186028 296507\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "SPLIT_RATE = 0.8\n",
    "data = df.values\n",
    "n_row, n_col = data.shape\n",
    "split_idx = int(n_row*0.8)\n",
    "data_train = data[:split_idx, :]\n",
    "data_valid = data[split_idx:, :]\n",
    "print(data_train.shape[0], data_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torchdata.Dataset):\n",
    "    def __init__(self, data, train=True):\n",
    "        n_row, n_col = data.shape\n",
    "        self.x = torch.FloatTensor(data[:, 1:]).contiguous().view(-1, n_col-1)\n",
    "        self.y = torch.FloatTensor(np.log(data[:, :1] + 1)).contiguous().view(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # 인덱스에 해당하는 데이터셋 리턴\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 데이터셋 수\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data=data_train)\n",
    "train_loader = torchdata.DataLoader(dataset=train_dataset,\n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   drop_last=True)\n",
    "valid_dataset = CustomDataset(data=data_valid)\n",
    "valid_loader = torchdata.DataLoader(dataset=valid_dataset,\n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, I, H, O):\n",
    "        super(Network, self).__init__()\n",
    "        self.I = I\n",
    "        self.H1 = H[0]\n",
    "        self.H2 = H[1]\n",
    "        self.O = O\n",
    "        \n",
    "        self.l1 = nn.Linear(self.I, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.H2)\n",
    "        self.l3 = nn.Linear(self.H2, self.O)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.H1)\n",
    "        self.bn2 = nn.BatchNorm1d(self.H2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bn1(self.l1(inputs))\n",
    "        outputs = self.activation(outputs)\n",
    "        outputs = self.bn2(self.l2(outputs))\n",
    "        outputs = self.activation(outputs)\n",
    "        \n",
    "        return torch.log(self.l3(outputs) + 1)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        outputs = self.forward(inputs)\n",
    "        return torch.exp(outputs) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(data_loader, model):\n",
    "    model.eval() # for batch norm at test time!\n",
    "    loss_function = nn.MSELoss(size_average=False)\n",
    "    losses=0\n",
    "    for i, (inputs, lg_targets) in enumerate(data_loader):\n",
    "        inputs, lg_targets = Variable(inputs).view(-1, I), Variable(lg_targets)\n",
    "        lg_outputs = model.forward(inputs)\n",
    "        losses += loss_function(lg_outputs, lg_targets).data[0]\n",
    "    return losses/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH=5\n",
    "LR=0.1\n",
    "I = 6\n",
    "\n",
    "model = Network(I, [50, 30], 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] [0/2316] mean_loss : 8.593\n",
      "[0/5] [500/2316] mean_loss : 0.531\n",
      "[0/5] [1000/2316] mean_loss : 0.481\n",
      "[0/5] [1500/2316] mean_loss : 0.477\n",
      "[0/5] [2000/2316] mean_loss : 0.478\n",
      "[1/5] [0/2316] mean_loss : 0.477\n",
      "[1/5] [500/2316] mean_loss : 0.470\n",
      "[1/5] [1000/2316] mean_loss : 0.466\n",
      "[1/5] [1500/2316] mean_loss : 0.465\n",
      "[1/5] [2000/2316] mean_loss : 0.463\n",
      "[2/5] [0/2316] mean_loss : 0.469\n",
      "[2/5] [500/2316] mean_loss : 0.462\n",
      "[2/5] [1000/2316] mean_loss : 0.457\n",
      "[2/5] [1500/2316] mean_loss : 0.457\n",
      "[2/5] [2000/2316] mean_loss : 0.458\n",
      "[3/5] [0/2316] mean_loss : 0.565\n",
      "[3/5] [500/2316] mean_loss : 0.459\n",
      "[3/5] [1000/2316] mean_loss : 0.453\n",
      "[3/5] [1500/2316] mean_loss : 0.453\n",
      "[3/5] [2000/2316] mean_loss : 0.455\n",
      "[4/5] [0/2316] mean_loss : 0.470\n",
      "[4/5] [500/2316] mean_loss : 0.450\n",
      "[4/5] [1000/2316] mean_loss : 0.453\n",
      "[4/5] [1500/2316] mean_loss : 0.451\n",
      "[4/5] [2000/2316] mean_loss : 0.452\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, (inputs, lg_targets) in enumerate(train_loader):\n",
    "        inputs, lg_targets = Variable(inputs).view(-1, I), Variable(lg_targets)\n",
    "\n",
    "        model.zero_grad()\n",
    "        lg_outputs = model.forward(inputs)\n",
    "        loss = loss_function(lg_outputs, lg_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        if i % 500 == 0:\n",
    "            print(\"[{0}/{1}] [{2}/{3}] mean_loss : {4:.3f}\"\\\n",
    "                  .format(epoch, EPOCH, i, len(train_loader), np.mean(losses)))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4514838425060501"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(valid_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = torch.FloatTensor(X_test).contiguous().view(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model.predict(Variable(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.317826 ],\n",
       "       [ 9.913862 ],\n",
       "       [26.39264  ],\n",
       "       ...,\n",
       "       [16.87075  ],\n",
       "       [15.803221 ],\n",
       "       [12.1100025]], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv', index_col='test_id')\n",
    "sub['price'] = pred.data.numpy()\n",
    "sub.to_csv('./data/torch_model.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
