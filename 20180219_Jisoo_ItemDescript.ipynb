{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis: Categories have different price distribution\n",
    "how well does item_description descript category?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_id                  0\n",
       "name                      0\n",
       "item_condition_id         0\n",
       "category_name          6327\n",
       "brand_name           632682\n",
       "price                     0\n",
       "shipping                  0\n",
       "item_description          4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cate_unique = df.loc[~df.category_name.isnull(), 'category_name'].unique()\n",
    "cate_dict = {cat: i for i, cat in enumerate(cate_unique, 1)}\n",
    "cate_dict[np.nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# category_id > doc_id\n",
    "# too slow\n",
    "# df['cate_id'] = df.category_name.replace(cate_dict)\n",
    "df['cate_id'] = df.category_name.map(cate_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data: 1482535\n",
      "null values: 4\n",
      "no description data: 82489\n"
     ]
    }
   ],
   "source": [
    "# issue1: find no descripted datas > not inclued in training \n",
    "print('total data:', len(df))\n",
    "print('null values:', df.item_description.isnull().sum())\n",
    "print('no description data:', df.item_description.isin(['No description yet']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5705"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do all data have categories when they have item_description?\n",
    "idx_bool = df.item_description.isin([np.nan, 'No description yet'])\n",
    "temp = df.loc[~idx_bool, ['cate_id', 'item_description']]\n",
    "temp.loc[temp.cate_id==0, 'cate_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cate_id             0\n",
       "item_description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs = df.loc[~idx_bool, ['cate_id', 'item_description']]\n",
    "df_docs = df_docs.loc[~(df_docs.cate_id == 0), :]\n",
    "df_docs.loc[df_docs.cate_id == 0, :].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first test cluster categories with names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import doc2vec\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import string as string_package\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file_path = './data/docs_name.txt'\n",
    "names = df.name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# issue2: '[rm]' \n",
    "# have to tokenized as a word\n",
    "def tokenize_sentence(sentence):\n",
    "    if '[rm]' in sentence:\n",
    "        if '[rm]' == sentence.strip():\n",
    "            return ['[rm]']\n",
    "        else:\n",
    "            sen_f, sen_b = sentence.split('[rm]')\n",
    "            sen_f = word_tokenize(sen_f.strip())\n",
    "            sen_b = word_tokenize(sen_b.strip())\n",
    "        return sen_f + ['[rm]'] + sen_b\n",
    "    else:\n",
    "        return word_tokenize(sentence)\n",
    "\n",
    "def remove_puncutation(sentence):\n",
    "    punc = list(string_package.punctuation) + ['``', '\"\"', \"''\"]\n",
    "    for p in punc:\n",
    "        sentence = sentence.replace(p, '')\n",
    "    return sentence\n",
    "\n",
    "def preprocess_sentence(sentence, stop_words=None):\n",
    "    if not stop_words:\n",
    "        stop_words = []\n",
    "    sentence = remove_puncutation(sentence)\n",
    "    temp = tokenize_sentence(sentence)\n",
    "    temp = [t.lower() for t in temp if t not in stop_words]\n",
    "    return temp\n",
    "\n",
    "def make_data_file(docs, stop_words=None):\n",
    "    assert type(docs) == list or np.ndarray, 'docs must be a list or np.array([something])'\n",
    "    \n",
    "    data = []\n",
    "    for sentence in tqdm(docs, desc='making data files', total=len(docs)):\n",
    "        temp = preprocess_sentence(sentence, stop_words=stop_words)\n",
    "        data.append(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making data files: 100%|██████████| 1482535/1482535 [03:05<00:00, 7976.25it/s]\n"
     ]
    }
   ],
   "source": [
    "data = make_data_file(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_data_file(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as out_file:\n",
    "        for tokens in data:\n",
    "            print('\\t'.join(tokens), file=out_file)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "write_data_file(data_file_path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as out_file:\n",
    "        for line in out_file:\n",
    "            data.append(line.strip().split('\\t'))\n",
    "    return data\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data_file(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_docs_name = pd.DataFrame({'cate_id': df.cate_id, 'doc_tokens': np.array(data)})\n",
    "# remove 6327 unknown categories\n",
    "df_docs_name_train = df_docs_name.loc[~(df_docs_name.cate_id == 0), :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_docs = [TaggedDocument(d, [i]) for i, d in (df_docs_name_train.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './data/model/cate_name.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(documents=tagged_docs, size=300, alpha=0.05, min_alpha=0.025, \n",
    "                        window=5, min_count=1, workers=3)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar_doc(query, cate_dict, model, stop_words=None):\n",
    "    cate_dict_inv = {v: k for k, v in cate_dict.items()}\n",
    "    query_vec = model.infer_vector(preprocess_sentence(query, stop_words=stop_words))\n",
    "    sims = model.docvecs.most_similar([query_vec], topn=5)\n",
    "    print('query: {}'.format(query))\n",
    "    print('='*5 + 'most similar docs' + '='*5)\n",
    "    for (tag, sim)in sims:\n",
    "        print('category: {0} | similarity: {1:.4f}'.format(cate_dict_inv.get(tag), sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df.loc[df.cate_id == 0, ['name', 'item_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: White Obey Long-Sleeve Shirt Size M\n",
      "=====most similar docs=====\n",
      "category: Vintage & Collectibles/Accessories/Handkerchief | similarity:0.7285\n",
      "category: Kids/Gear/Playard Bedding | similarity:0.7244\n",
      "category: Handmade/Candles/Sticker | similarity:0.7129\n",
      "category: Handmade/Woodworking/Accessories | similarity:0.7120\n",
      "category: Vintage & Collectibles/Furniture/Entertainment | similarity:0.7106\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(len(df_test))\n",
    "query = df_test.values[idx][0]\n",
    "get_similar_doc(query, cate_dict, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using item description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cate_id</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Banana republic bottoms, Candies skirt with ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cate_id                                   item_description\n",
       "1        2  This keyboard is in great condition and works ...\n",
       "2        3  Adorable top with a hint of lace and a key hol...\n",
       "3        4  New with tags. Leather horses. Retail for [rm]...\n",
       "4        5          Complete with certificate of authenticity\n",
       "5        6  Banana republic bottoms, Candies skirt with ma..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making data files: 100%|██████████| 1394337/1394337 [05:21<00:00, 4334.64it/s]\n"
     ]
    }
   ],
   "source": [
    "data_file_path = './data/docs_item.txt'\n",
    "items = df_docs.item_description.values\n",
    "data = make_data_file(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "write_data_file(data_file_path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data_file(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_docs_cate_train = pd.DataFrame({'cate_id': df_docs.cate_id, 'doc_tokens': np.array(data)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_docs = [TaggedDocument(d, [i]) for i, d in (df_docs_cate_train.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './data/model/cate_item.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(documents=tagged_docs, size=300, alpha=0.05, min_alpha=0.025, \n",
    "                        window=5, min_count=1, workers=3)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df.loc[df.cate_id == 0, ['name', 'item_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: PREMIUM NFL FOOTBALL HOT PACK! 3 HITS!\n",
      "query: Each HOT PACK includes the following. -1 to 2 Autographs -1 to 2 Jersey or Relic Cards -1 to 2 Numbered Insert or Parallels -10 Rookie Cards (all current players) -10 Base Cards You will definitely get more than [rm] value in this pack! I am trying to get rid of my collection and I don't have time to post every card. Players that could be included in the hot packs are: Brett Favre, Julian Edelman, Allen Robinson, Chad Johnson and more! [rm] SHIPPED FOR EACH PACK Let me know if you want more than one hot pack and i can lower the price to save on shipping! :) THIS IS A GREAT DEAL!! 3 HITS FOR [rm]\n",
      "=====most similar docs=====\n",
      "category: Home/Bath/Bathroom Shelves | similarity: 0.6268\n",
      "category: Other/Automotive/Oils & Fluids | similarity: 0.6057\n",
      "category: Handmade/Woodworking/Boxes | similarity: 0.6045\n",
      "category: Handmade/Woodworking/Accessories | similarity: 0.6007\n",
      "category: Handmade/Ceramics and Pottery/Coasters | similarity: 0.5835\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(len(df_test))\n",
    "query = df_test.values[idx][1]\n",
    "print('name: {}'.format(df_test.values[idx][0]))\n",
    "get_similar_doc(query, cate_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
